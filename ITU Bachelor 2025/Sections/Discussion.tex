\section{Discussion}
In this section we will touch back on the stated sub-research-questions and discuss what value the applied method deliver when trying to answer these, and finally we discuss our initial research question and look at the bigger picture by looking at what initially motivated the paper and how this paper contributes in that regard.
\subsection{Q1 Discussion}
\label{sec:DiscussionQ1}
"\textit{\subquestionI}"
During development of DocTide several challenges and limitations was encountered in the quest to achieve the behavior of an autonomous agent on the hardware which the GitHub Runner provided. This section will discuss how these hardware limitation influenced the design of DocTide when trying to follow the proposed design of an LLM-based autonomous Agent from Wang et al.\cite{wang2024survey}.
\subsubsection*{Letting the LLM do it all}
The first approach was to find an LLM which fitted the hardware limitation, and as one of the first suggestions we met on Ollama was the llama 3.2 with a size of 2 gb in memory. Our intuition was that this size wouldn't exhorts the 14 gb RAM (\Cref{sec:githubRunner}), and therefor should be able to run fast. And when provided the whole file, on max 20 line, it did its job fairly good. But when we got our real world evaluation framework (\Cref{sec:DocTideLabs}) set up, it medially exposed the small LLMs limits when having to deal with file sizes of upwards to 600 lines (\Cref{appendix:A}). It is not possible to have a small LLM understand the whole context and write the whole file from scratch, hoping it have made sensible function level documentation, and not broken the code. 
\subsubsection*{Identifying the modified functions to document}
But since we had to stick with the small LLM, a way to offload the LLMs work, was to analyze the context on its behalf using tree-sitter and diff\_lib. If we should have stayed more true to the proposed agent design by Wang et al.\cite{wang2024survey} we should probably have tried to make a \textcolor{red}{LLM-drive planing module,|action?} which have identified and served the modified functions to an LLM-driven action which then could have generate the function level documentation. But since our level of trust in the small LLM to understand such large files had decreased, we opted for gaining more trust to the system by using static analysis. But this was not the only place we had to cut down the agents autonomy and using the content knowledge tree-sitter provides.
\subsubsection*{Write documentation back into the source code}
The test environment also exposed how the number of modified functions increased the actions execution time.
Execution times came upwards to 3 hours on merge commits, where multiply files was commit at once, and when investigating which process in the action that took so much time, and it was obvious that it was when the LLM had to write whole files and try to add the function level documentation the correct place. The approach with AST was further used to figure out where exactly a function level comment should be place given a specific modified function, relieving the LLMs task to only generate the function level documentation, provided the code of the modified function, and pass it to the insert functionality which takes the calculated byte offset to where the documentation should be, from the context of the AST, and insert it into the source code. The decreased the execution time significantly, but again we opted to go with an static analysis approach, rather than making a new LLM-based action which only purpose was to figure out where the most appropriate location for the documentation would be. The approach chosen gave us the most trust in that the agent would place the documentation at the right location, but we in turn decreased its autonomous abilities. And in fact, letting an LLM asses where it is appropriate to place a function-level documentation would make it more portable and not limited only identify where a comment should be placed given the supported programming language or level of documentation.
\subsubsection*{Ci integration}


\subsection{Q2 Discussion}
\label{sec:DiscussionQ2}
"\textit{\subquestionII}"
A semantic similarity score should, opposite to just looking at character comparison, give a quantitative measure for how much of the semantic meaning is present between two texts. This is why we chose to use the semantic similarity model from Sentence Transformers to compare the similarity between function-level documentation created by a human developer and by DocTide, to get a understanding of how similar to what the human developer intent to explain in a function-level documentation DocTide manages to generate. And in our qualitative analysis of the generated semantic similarity score(\Cref{sec:identifying thresholds}) we found this models scores to be a good indicate for this measurement. But to answer the sub-question \textbf{Q2} the measurement alone falls a bit short. Even though this is a great indication for what kind of value you can expect a system like DocTide to delivery in terms of the 


But with DocTides limited context understanding, only the source code of the function, 
\subsection{Q3 Discussion}
\label{sec:DiscussionQ3}
"\textit{\subquestionIII}"
The results of the experiment as described in \cref{sec:suc_results} shows a \textit{'success\_rate'} for DocTide at \textit{68.57\%}, meaning that \textit of the time the DocTide agent has attempted to generate documentation, it has resulted in documentation following the format described in \cref{sec:method} which has been inserted in the code. 
\subsubsection*{Current documentation coverage}
To discuss the implications of this, the python tool \textit{'docstr-coverage'}\footnote{\url{https://pypi.org/project/docstr-coverage/}} is ran on the same repository as the experiment was executed. This tool scans the entire codebase of the repository, identifies possible locations for method-level documentation, and meassures this coverage of these. As seen in \cref{lst:doc_percent_flask} this shows that the repository has method level documentation coverage of \textit{38.7\%}, implying that if the same project was ran with only DocTide handling documentation, the project would have a higher coverage of method level documentation than its current state.

\begin{lstlisting}[language=sh, label={lst:doc_percent_flask},      caption= Ratio of method level documentation in flask repository]
    Needed: 8193  -  Found: 3169  -  Missing: 5024
    Total coverage: 38.7%  -  Grade: Not good
\end{lstlisting}
\begin{itemize}
    \item The score reflects that the agent only has one attempt. (vs. reprompting it self)
    \item Discuss common pitfalls of the metric, se på stikprøver. Måske bind sammen med den før, hvis der er common pitfalls, så kunne dette måske integreres ind i reprompting
\end{itemize}
\subsection{Overall discussion}
Remember to use motivation from introduction


Level of autonomy: We set of with a medium level of autonomy, but with the limited size/capabilities of the LLM model, and the vitality of not injecting working code into the code base, we limited the level of autonomy to a minimum. With more careful approaches, this level could be rissen, by letting the LLM asses its own output, and make it evaluate if it is a satisfactory documentation of the code.

\subsection{Future work}
\begin{itemize}
    \item more intent 
        \item persistent memory with config files, both users of DocTide can use to define some overarcing concepts, and to store learnings made from modifying in PRs
        \item 
\end{itemize}