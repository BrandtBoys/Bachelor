\section{Discussion}
In this section we will touch back on the stated sub-research-questions and discuss what value the applied method deliver when trying to answer these, and finally we discuss our initial research question and look at the bigger picture by looking at what initially motivated the paper and how this paper contributes in that regard.
\subsection{Integrating an autonomous Agent into GitHub}
\label{sec:DiscussionQ1}
"\textit{\subquestionI}"
During development of DocTide several challenges and limitations was encountered in the attempt to achieve the behavior of an autonomous agent on the hardware which the GitHub Runner provides. This section will discuss how these hardware limitation influenced the design of DocTide when trying to follow the proposed design of an LLM-based autonomous Agent from Wang et al.\cite{wang2024survey}.
\subsubsection*{Letting the LLM do it all}
The first approach was to find an LLM which fitted the hardware limitation, and as one of the first suggestions we met on Ollama was the llama 3.2 with a size of 2 gb in memory. Our intuition was that this size wouldn't exhaust the 14 gb RAM (\Cref{sec:githubRunner}), and therefor should be able to run fast. And when provided the whole file, on max 20 line, it did its job fairly good. But when we got our real world evaluation framework (\Cref{sec:DocTideLabs}) set up, it immediately exposed the small LLMs limits when having to deal with file sizes of upwards to 600 lines. As seen in \Cref{appendix:A} a snippet of the incoming commit with documentation generated by DocTide, had reduced the file from 661 lines to 155. Further it had provided the comment in a markdown format (line 153) and had made an informative, but code breaking, description of its incompetent contribution. This showed us that it is not possible to have a small LLM understand the whole context and write the whole file from scratch, hoping it have made sensible documentation, and not broken the code. 
\subsubsection*{Identifying the modified functions to document}
But since we had to stick with the small LLM, a way to offload the LLMs work, was to analyze the context on its behalf using tree-sitter and diff\_lib. If we should have stayed more true to the proposed agent design by Wang et al.\cite{wang2024survey} we should probably have tried to make a \textcolor{red}{LLM-drive planing module,|action?} which have identified and served the modified functions to an LLM-driven action which then could have generate the function level documentation. But since our level of trust in the small LLM to understand such large files had decreased, we opted for gaining more trust to the system by using static analysis. But this was not the only place we had to cut down the agents autonomy and using the content knowledge tree-sitter provides.
\subsubsection*{Write documentation back into the source code}
The test environment also exposed how the number of modified functions increased the actions execution time.
Execution times came upwards to 3 hours on merge commits, where multiply files was commit at once, and when investigating which process in the action that took so much time, and it was obvious that it was when the LLM had to write whole files and try to add the function level documentation the correct place. The approach with AST was further used to figure out where exactly a function level comment should be place given a specific modified function, relieving the LLMs task to only generate the function level documentation, provided the code of the modified function, and pass it to the insert functionality which takes the calculated byte offset to where the documentation should be, from the context of the AST, and insert it into the source code. The decreased the execution time significantly, but again we opted to go with an static analysis approach, rather than making a new LLM-based action which only purpose was to figure out where the most appropriate location for the documentation would be. The approach chosen gave us the most trust in that the agent would place the documentation at the right location, but we in turn decreased its autonomous abilities. And in fact, letting an LLM asses where it is appropriate to place a function-level documentation would make it more portable and not limited only identify where a comment should be placed given the supported programming language or level of documentation.
\subsubsection*{Ci integration}


\subsection{Q2 Discussion}
\label{sec:DiscussionQ2}
"\textit{\subquestionII}"
A semantic similarity score should, opposite to just looking at character comparison, give a quantitative measure for how much of the semantic meaning is present between two texts. This is why we chose to use the semantic similarity model from Sentence Transformers to compare the similarity between function-level documentation created by a human developer and by DocTide, to get a understanding of how similar to what the human developers intent to explain in a function-level documentation DocTide manages to generate. And in our qualitative analysis of the generated semantic similarity score(\Cref{sec:identifying thresholds}) we found this models scores to be a good indicate for this measurement. This measurement does not cover all attributes of documentation and does therefor not cover the all the ways to asses to what level DocTide is able to produce function-level documentation compared to that of a human developer, but it gives an indication on one of the more humane attributes by looking at what level DocTide captures the same intent with the documentation as the developer does.
Reflecting upon why the score was as it was, when looking at some of the data there were a correlation between poorly scored documentations and the developer documentation containing context which was derived from other files or how the function was used and not only its implementation. 
This could hypothesize that introducing more context to DocTide would improve its semantic scores, this hypothesis could be further motivated by having a developer create function-level documentation given only the source code of the function, operating under the same restrictions as DocTide, and see if the developers semantic score would follow the same distribution, indicating that it is the lack of context that is a factor for the level of similarity or if the developer scored higher, indicating that human intuition is a driving factor.
\subsection{Q3 Discussion}
\label{sec:DiscussionQ3}
"\textit{\subquestionIII}"
The results of the experiment as described in \cref{sec:suc_results} shows a \textit{'success\_rate'} for DocTide at \textit{68.99\%}, meaning that \textit{68.99\%} of the time the DocTide agent has attempted to generate documentation, it has resulted in documentation following the format described in \cref{sec:method} which has been inserted back into the code base. 
\subsubsection*{Current documentation coverage}
To discuss the implications of this, the python tool \textit{'docstr-coverage'}\footnote{\url{https://pypi.org/project/docstr-coverage/}} is ran on the same repository as the experiment was executed. This tool scans the entire codebase of the repository, identifies possible locations for method-level documentation, and meassures this coverage of these. As seen in \cref{lst:doc_percent_flask}, the results of this shows that the repository has method level documentation coverage of \textit{38.7\%}, implying that if the same project was ran with only DocTide handling documentation, the project would have a higher coverage of method level documentation than its current state. Though as found in \cref{sec:sem_results} not with the same level of information conveyed in each comment.

\begin{lstlisting}[language=sh, label={lst:doc_percent_flask},      caption= Ratio of method level documentation in flask repository]
    Needed: 8193  -  Found: 3169  -  Missing: 5024
    Total coverage: 38.7%  -  Grade: Not good
\end{lstlisting}

\subsubsection*{Format failures}
When looking at random samples collected from the 31.01\% of the dataset where the documentation generated does not follow the format described in \cref{sec:method}, two common reasons stands out as recurring reasons to format failure: the agent either misses the set of quotes actively 'closing' the comment, or the agent returns a copy of the method for which it is generating documentation as a prefix or suffix to the documentation itself. 

DocTide follows a very conservative approach where the attempt at generating documentation for a given function, is aborted when documentation not following the correct format is produced. After identifying the pattern of two common recurring reasons for format failure, it is hypothesized that integrating some form of internal feedback mechanism biased towards handling these exact cases could lead to an improvement in the 'success\_rate' metric. Similar to what is discussed in \cref{sec:DiscussionQ1}, this is limited by the capabilities of the LLM leading to a tradeoff between autonomy and trust.

\subsection{Overall discussion}
Following the design for LLM-based autonomous agents proposed by Wang et al.\cite{wang2024survey} has provided great direction and considerations on how to extend the autonomy of the agent and practical approaches to how to achieve that. However the development and integration of DocTide as a LLM-based autonomous agent has revealed a clear bias in our developing approach, which has shown evident in the solutions we resolved to when facing the challenges described in \Cref{sec:DiscussionQ1}. With a mindset of software developers working towards a working product, the limited size/capabilities of the LLM model the integration into GitHub Actions required, and the vitality of not injecting working code into the code base let to the identification of a notion of tradeoff between trust and autonomy. This notion was not something we found in research prior to the development of DocTide, but has show to be a beneficial cornerstone for the practical development of autonomous agents that aims to integrate into restriction of existing systems. For the development of DocTide, this resulted in the level of autonomy being scaled back to a minimum in order to ensure predictable behavior. This scale back and working with a more simple goal as we have done, with many of their concepts being derived from quiet complex systems, it have be challenging to keep a clear separation of the proposed modules when applying the theory into practice. Especial since their design relied on every module being driven by an LLM, and our prototype only using one instance of an LLM the lines became more blurred to the point where we decided to see the planing module and the action module as one, since it followed a sequential execution, and all actions followed the pre-defined plan. The level of trust in the capabilities of the LLM's utilized in the autonomous agent has shown to be what dictates the obtainable autonomy of the agent. As the field of LLM's and their capabilities currently is experiencing rapid evolution, it is therefore hypothesized that the boundaries and limitation of developing integrated autonomous agents capable of automating the task of making software documentation as well will be pushed. Why still choosing to follow this design proposal came to the potential that it offers, and how very few steps from the version DocTide is in now, will utilize the benefits of the structure.
\\\\
As stated in the \cref{sec:intro}, missing or outdated documentation as a result of developers lacking time is identified as an important issue that is frequently encountered. 

With 78.14\% of the documentation generated by DocTide achieving \textit{medium-} and \textit{high similarity} to documentation produced by human developers, and a \textit{'success rate'} of 68.99\% on generating and reintegrating documentation in the correct format, DocTide as a prototype manages to support the claim that an integrated autonomous agent would be able to mitigate the severity of the missing or outdated documentation. The direct integration into GitHub actions enables these benefits without the need of human interference, changing the state of documentation from opt-in to opt-out.

\subsection{Future work}
We will here describe which next step for DocTide we would be interested to investigate:
\subsubsection*{Gaining more context}
As mentioned in \Cref{sec:DiscussionQ2}, increasing DocTides context of the source code and the context in which the code is would maybe be a factor to increasing its level of similarity. This could simply be done in giving it the code of the whole file in which the function is situated in, or make an dependency analysis to give give more knowledge of the surrounding context.
\subsubsection*{Let the agent validate its response}
As discussed in \cref{sec:DiscussionQ3} it is hypothesized that implementing a feedback mechanism to enable re-attempting the generation of documentation after failing on format in the first attempt, could lead to an increase in the \textit{'succes rate'} of the format reliability.

It is therefore proposed that future work implement \textit{model feedback} as described by Wang et al.\cite{wang2024survey} where internally an LLM is used to decide whether the generated documentation follows correct format and to perform \textit{re-prompting} in the case that it does not.

\subsubsection*{Enable DocTide to evolve based on feedback from Pull Requests}
As developers will be prompted with a pull request, containing DocTides generated function level documentation, they will either accept it, modify it or reject it. A future work could be to find a way to harvest this valuable feedback, to enable DocTide to learn and evolve by saving its learning to a persistent memory structure.