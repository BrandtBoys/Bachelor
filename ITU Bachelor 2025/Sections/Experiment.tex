\section{Experiment}
\label{sec:exp}
To investigate \textbf{Q2} and \textbf{Q3} stated in \Cref{sec:method} an experiment is conducted, utilizing the evaluation framework DocTide Labs as described in section \ref{sec:DocTideLabs}. 
\\ \\
The evaluation is conducted using the opensource repository 'flask'\footnote{\url{https://github.com/pallets/flask}} as this is a well maintained opensource python repository, recognized for following a practice of method level documentation. 500 entries from the commit history ranging from \textit{'cabda59'} to \textit{'f61172b'} has been used as the commits to replicate during the evaluation.

As described in \cref{sec:method} two metrics is collected through this experiment: \textit{'semantic similarity} and \textit{'success rate'}

\subsection{Semantic Similarity}
Through the experiment when the DocTide agent generate documentation for a piece of code that has original documentation generated by a human developer, these pairs of documentation is collected. The Python module Sentence Transformers\footnote{\url{https://www.sbert.net/examples/cross_encoder/applications/README.html}} is then used to calculate the semantic similarity through running a Cross Encoder model.

Taking into considerations the efficiency of running this model over large quantities of comment pairs, we have decided to use the \textit{'stsb-roberta-base'} model, which scores 90.17 in the STSbenchmark\footnote{\url{https://sbert.net/docs/cross_encoder/pretrained_models.html}}.
\subsubsection{Identifying thresholds}
\label{sec:identifying thresholds}
Running the experiment resulted in the collection of \textbf{215} unique sets of documentation pairs and their semantic score, the distribution of which is seen in \cref{fig:sem_hist}.
\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{Figures/semantic_score_histogram.png}
\caption{Histogram showing the normal distribution of the semantic-score metric collected}
\label{fig:sem_hist}
\end{figure}

Before being able to infer any information from this result, the result is processed in order to qualitatively identify the thresholds between the three quality buckets \textit{'Low-'}, \textit{'Medium-'} and \textit{'High similarity'} as described in section \cref{sec:method}.:
\\\\
First a \textbf{sample} is taken from the result pairs, which all contain the original documentation, the agent generated documenation and the semantic score. As seen in \cref{fig:sem_hist} the results follows a normal distribution, so the \textbf{sample} is taken to be representing of this, by utilizing \textit{stratified sampling}\footnote{\url{https://www.geeksforgeeks.org/stratified-sampling-in-pandas/}}. The column containing the semantic score is then hidden, and each pair is qualitatively inspected by comparing the original documentation to the agent generated documentation, before finally being assigned to one of the three quality buckets. For the entire results of this categorization of the sample, see table in \cref{appendix:sample_table}.

Next the \textbf{sample} is grouped by the assigned buckets, and summary statistics is calculated, before finally the thresholds is calculated as the midpoint between the maximum and minimum value of the higher and lower bucket respectively.:
\[
Low\_to\_medium = \frac{Max_{low}+Min_{medium}}{2} = 0.5590
\]
\[
Medium\_to\_high = \frac{Max_{medium}+Min_{high}}{2} = 0.6460
\]

\subsubsection{Results}
In \cref{fig:sem_box} the results of the experiment is plotted against the quality thresholds as identified in \cref{sec:method}, and from this it is shown that the majority of the data points falls within the buckets of \textit{medium-} and \textit{high similarity}, with the bucket of \textit{medium similarity} as the must frequent with more than 50\% of the data points.

\label{sec:sem_results}
\begin{figure}[H]
\centering
\includegraphics[width=0.7\linewidth]{Figures/semantic_score_box_plot.png}
\caption{Box plot visualizing the summary-statistics of the semantic scores against the identified thresholds.}
\label{fig:sem_box}
\end{figure}

\noindent
The exact distribution across the identified thresholds is as follows: 78.14\% of the results has a semantic score above the low threshold, and 22.79\% is above the high threshold, leaving 55.35\% to fall in the bucket of \textit{medium similarity} between the two thresholds.

\subsection{Format Reliability}
When running the DocTide agent with the 'testing' flag set to true, all documentation generation attempts is collected.
Through the experiment, this is collected by a validation function \textit{validate\_response\_as\_comment()}, which is run for every response created. This function analyze the response from the LLM by creating a AST of the response using Tree-sitter\footnote{\url{https://tree-sitter.github.io/tree-sitter/}}, and then check that all nodes is of type comment.
We calculate the \textit{success rate} by counting all successfully formatted responses and all possible responses and then simply calculate the ratio between those two numbers.
\subsubsection{Reliability results}
\label{sec:suc_results}
Running the agent over the commits described for the experiment the DocTide agent attempted to generate documentation \textbf{945} times, out of which \textbf{652} were in the correct format. This means that 652 times the documentation generated by DocTide was able to re-integrate into the code base. This corresponds to a success\_rate of:
\[
Success\_rate=\frac{652}{945}*100\% = 68.99\%
\]

\subsection{Limitiations / Threats to validity}
The notion of success that we described in the method \Cref{sec:method} was build on knowledge obtained in the early stage of development, when we encountered that the LLM returned comments in markdown format, with the function definition at the beginning, with explanatory comments on what it has made, all formats which is not integrable into source code. Therefor we constructed this measurement of success based on the agents ability to generate documentation which would be integrable.

A potential thread to the internal validity of the semantic similarity is that it is the authors of this paper who made the qualitative comparison, when assessing which level of similarity there is between the two documentations in \Cref{sec:identifying thresholds}. One could speculate if they were biased to try to make it follow the semantic score created by the Cross Encoder model, which would make their results more significant and inferring a correlation between the semantic score and the qualitative similarity analysis. 
\\ \\
The following error thrown under the execution of the testing framework has not been resolved, leading to the execution of the experiment sometimes crashing at arbitrary times, and thus the data has been collected over multiple runs.:
\begin{lstlisting}[language=bash, label={lst:unresolved_errors}, caption=Unresloved error ]
    github.GithubException.UnknownObjectException: 404 {"message": "Not Found", "documentation_url": "https://docs.github.com/rest/repos/contents#get-repository-content", "status": "404"}
\end{lstlisting}