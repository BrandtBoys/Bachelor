\newlist{qlist}{itemize}{1}
\setlist[qlist]{label=\textbf{Q1:}}
\newcommand\itemb{\item[\textbf{Q2:}]}
\newcommand\itemc{\item[\textbf{Q3:}]}

\section{Methodology}
\label{sec:method}
As stated in the introduction the contribution of this paper is to investigate the following:
\begin{quote}
    \researchQuestion
\end{quote}
The methodology used to do so, is to first identify the following three sub-questions, to clarify the challenges and limitations encountered by our approach and to evaluate on how well it works, to give an easy intuition on what this approach to autonomous code documentation agents offers, the sub-questions is as follows:

\begin{qlist}
    \item \subquestionI\textbf{:}
    Looking at what considerations one should take into account when designing an autonomous agent under the restrictions of integrating said agent into a github environment, and how challenges faced influences the proposed design.
    \itemb \subquestionII\textbf{:}
    The capabilities to generating function-level documentation of DocTide is compared to that of a human developer, since the intend behind developing DocTide as a LLM-based autonomous agent, is to automate the task of creating function-level documentation to a level comparable to that of a human developer.
    \itemc \subquestionIII\textbf{:}
    Since the core functionality of DocTide is to generate code which can be injected into the source code, and become function level source code documentation, it is necessary that both the syntax is correct such that the code can compile and that only documentation is created, and not executable code.
\end{qlist}


To investigate \textbf{Q1}, a prototype of an autonomous agent, DocTide is developed, following the design described in section \ref{sec:BackgroundAgentDesign}, to integrate into Github Actions. The results of this is presented in section \ref{sec:DesignDocTide}, and discussed in section \ref{sec:DiscussionQ1}. The \textit{'human similarity metric'} and \textit{'success metric'} described in the \textit{LLM-based autonomous agent evaluation} section in Wang et al.\cite{wang2024survey} was identified as appropriate measures to address \textbf{Q2} and \textbf{Q3} respectively. These metrics are collected by running an experiment described in section \ref{sec:exp}, using a reusable evaluation framework, DocTide Labs (Section \ref{sec:DocTideLabs}). DocTide Labs was designed to facilitate the seamless integration of DocTide into a repository and to enable the systematic collection of data for the specified metrics.

The \textit{`human similarity metric'} is defined by how similar to human documentation DocTide creates. This is scored using the CossEncoder from sentence transformers\footnote{\url{https://sbert.net/examples/cross_encoder/applications/README.html}} to get a semantic score between existing function level documentation and function level documentation created be DocTide. The results of this is presented in section \ref{sec:sem_results}, and discussed in section \ref{sec:DiscussionQ2}.

The core functionality of DocTide is to be able to inject function level documentation into source code, this has to be correctly formatted, otherwise malicious code, unintended or code braking behavior could be introduced. At a minimum, the documentation generated has to be of a valid comment type for the given programming language. Therefore the \textit{'success metric} is defined as the ratio between the number off successful formatted function level documentations generated by DocTide and the total number of attempted documentations. The results of this is presented in section \ref{sec:suc_results}, and discussed in section \ref{sec:DiscussionQ3}.

\subsection{AI usage disclaimer}
\textit{During the development of the code base for this project, openAI's ChatGPT 4o has been used as a tool for enhancing our software development. It has been used to understand the used frameworks and technologies as well as to understand error messages, but has not been used to generate code verbatim for the code base.
}